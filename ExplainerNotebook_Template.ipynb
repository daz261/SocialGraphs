{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "name": "ExplainerNotebook_Template.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daz261/SocialGraphs/blob/main/ExplainerNotebook_Template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bca7b0f"
      },
      "source": [
        "Example Jupyter Notebook: https://github.com/Nab-88/social-graphs-and-interactions/blob/master/notebook/final_version.ipynb \n",
        "\n",
        "### Send notebook on the Messenger chat when done with your part with NAME and TIMESTAMP\n",
        "\n",
        "### Imports: we agreed on importing most of our functions from the code pushed in the Git repo\n",
        "\n",
        "\n",
        "### 1. Motivation (ARTUR)\n",
        "# What is your dataset?\n",
        "The dataset we use is combined from multiple sources, where Wikipedia plays the main role and other are used to enrich the primary dataset.\n",
        "\n",
        "## Primary data source\n",
        "The core part of the dataset is downloaded from the Wikipedia. We use Wikipedia API to create a list of artists and albums, for which we later look for links to other artists that these refere to. Its consequence is that the relationships we capture in the collaboration network include various types of interactions, e.g.:\n",
        "- actual collaborations in which musicians were taking part of creating others album (playing instruments, singing or producing them),\n",
        "- song writers, for example if album includes a cover\n",
        "- inspirations\n",
        "- former bands of the album authors\n",
        "\n",
        "The code we used to download this data is [TODO:link to code] and its output is available [TODO:link to download artist csv]\n",
        "\n",
        "To enrich the network we are using secondary data sources:\n",
        "\n",
        "## Secondary data sources\n",
        "### **Spotify API**\n",
        "For each song in collected albums we use Spotify API [available here](https://developer.spotify.com/documentation/web-api/) to gather their audio features. These include:\n",
        "\n",
        "##### **acousticness**\n",
        "A confidence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high confidence the track is acoustic\n",
        "##### **danceability**\n",
        "Danceability describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable.\n",
        "##### **energy**\n",
        "Energy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale. Perceptual features contributing to this attribute include dynamic range, perceived loudness, timbre, onset rate, and general entropy.\n",
        "##### **instrumentalness**\n",
        "Predicts whether a track contains no vocals. \"Ooh\" and \"aah\" sounds are treated as instrumental in this context. Rap or spoken word tracks are clearly \"vocal\". The closer the instrumentalness value is to 1.0, the greater likelihood the track contains no vocal content. Values above 0.5 are intended to represent instrumental tracks, but confidence is higher as the value approaches 1.0.\n",
        "##### **liveness**\n",
        "Detects the presence of an audience in the recording. Higher liveness values represent an increased probability that the track was performed live. A value above 0.8 provides strong likelihood that the track is live.\n",
        "##### **speechiness**\n",
        "Speechiness detects the presence of spoken words in a track. The more exclusively speech-like the recording (e.g. talk show, audio book, poetry), the closer to 1.0 the attribute value. Values above 0.66 describe tracks that are probably made entirely of spoken words. Values between 0.33 and 0.66 describe tracks that may contain both music and speech, either in sections or layered, including such cases as rap music. Values below 0.33 most likely represent music and other non-speech-like tracks.\n",
        "##### **tempo** \n",
        "The overall estimated tempo of a track in beats per minute (BPM). In musical terminology, tempo is the speed or pace of a given piece and derives directly from the average beat duration.\n",
        "##### **valence** \n",
        "A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry).\n",
        "\n",
        "All audio feature descriptions are following the API's [description](https://developer.spotify.com/documentation/web-api/reference/#/operations/get-several-audio-features).\n",
        "\n",
        "### **Billboard**\n",
        "We have used [this Kaggle dataset](https://www.kaggle.com/danield2255/data-on-songs-from-billboard-19992019) to get information about popularity of the albums, as measured by Billboard magazine charts. Billboard publishes lists of relative popularity of albums and songs each week. It is US-based, but they aim to be global.\n",
        "\n",
        "The features we use are:\n",
        "##### **last week** \n",
        "Weeks since the album has been in Billboard charts last time (measured since the point of the Kaggle dataset creation)\n",
        "\n",
        "##### **weeks on chart** \n",
        "Number of weeks the album had any of its song in Billboard charts.\n",
        "\n",
        "##### **peak rank** \n",
        "The highest rank the album has reached in the charts.\n",
        "\n",
        "### **Genius**\n",
        "We used portal Genius to extract songs lyrics, which we later use for TF-IDF and sentiment analysis.\n",
        "\n",
        "\n",
        "\n",
        "Why did you choose this/these particular dataset(s)?\n",
        "What was your goal for the end user's experience?"
      ],
      "id": "0bca7b0f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8760789"
      },
      "source": [
        "### 2. Basic stats (ARTUR)\n",
        "#### Write about your choices in data cleaning and preprocessing\n",
        "\n",
        "\n",
        "#### Write a short section that discusses the dataset stats (here you can recycle the work you did for Project Assignment A)"
      ],
      "id": "a8760789"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnFinN87uDVv"
      },
      "source": [
        "from typing import List\n",
        "import requests\n",
        "import tqdm\n",
        "#from bs4 import BeautifulSoup\n",
        "\n",
        "import os.path\n",
        "from os.path import isfile, join\n",
        "#import nltk\n",
        "import community\n",
        "from community import community_louvain\n",
        "import pandas as pd\n",
        "import os\n",
        "#from util import DATA_PATH\n",
        "from os import listdir\n",
        "\n",
        "from os.path import isfile, join\n",
        "\n",
        "import numpy as np\n",
        "#from get_spotify_access import refresh_spotify_access\n",
        "import zipfile\n",
        "#import vaderSentiment\n",
        "#import spotipy\n",
        "#from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "import os\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from os.path import isfile, join\n",
        "import regex as re\n",
        "from tqdm import tqdm\n",
        "import zipfile\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import networkx as nx\n",
        "sns.set()"
      ],
      "id": "LnFinN87uDVv",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "bdGm_tyriRrp",
        "outputId": "1684d358-a74b-436d-b0b3-c1b19b1e9238"
      },
      "source": [
        "f = pd.read_csv(\"final_df.csv\")"
      ],
      "id": "bdGm_tyriRrp",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-704319a7c771>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"final_df.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'final_df.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3375bf7c"
      },
      "source": [
        "### 3.Tools, theory and analysis (ALL OF US)\n",
        "\n",
        "Talk about how you've worked with text, including regular expressions, unicode, etc.\n",
        "\n",
        "Describe which network science tools and data analysis strategies you've used, how those network science measures work, and why the tools you've chosen are right for the problem you're solving.\n",
        "\n",
        "How did you use the tools to understand your dataset?\n",
        "\n",
        "Explain the overall idea\n",
        "1. Analysis step 1\n",
        "2. Explain what you're interested in\n",
        "3. Explain the tool\n",
        "4. Apply the tool\n",
        "5. Discuss the outcome"
      ],
      "id": "3375bf7c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2c67c4c5"
      },
      "source": [
        "### 3.1 Text Analysis and Word Clouds (NIKOLAJ)"
      ],
      "id": "2c67c4c5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63258eb0"
      },
      "source": [
        "### 3.2 Sentiment Analysis (DARIA)\n",
        "\n",
        "The purpose of sentiment analysis is to analyze the lyrics of different artists and albums in order to determine if specific features such as lexical richness, word repetition or the use of specific words can be used to predict the most successful artists and their albums.\n",
        "\n"
      ],
      "id": "63258eb0"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ziy0I2J7vsTz"
      },
      "source": [
        ""
      ],
      "id": "ziy0I2J7vsTz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "038780d2"
      },
      "source": [
        "### 3.3 Machine Learning Predictions (ARTUR)"
      ],
      "id": "038780d2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "097afbe0"
      },
      "source": [
        "### 4. Discussion. (DARIA)\n",
        "What went well?,\n",
        "What is still missing? What could be improved?, Why?\n",
        "Contributions. Who did what?\n",
        "\n",
        "### 5.Contributions.  (ALL OF US)\n",
        "\n",
        "### 6. References.  (ALL OF US)\n",
        "We have imported most of the function in our explainer notebook from Python files in our project repository: https://github.com/daz261/SocialGraphs \n",
        "\n",
        "### 7. Appendix.  (ALL OF US)"
      ],
      "id": "097afbe0"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5261b75a"
      },
      "source": [
        ""
      ],
      "id": "5261b75a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58d1d11b"
      },
      "source": [
        ""
      ],
      "id": "58d1d11b",
      "execution_count": null,
      "outputs": []
    }
  ]
}